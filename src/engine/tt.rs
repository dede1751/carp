/// Implement a Transposition Table, used to store already searched positions for future use.
use std::{
    mem::{size_of, transmute},
    sync::atomic::{AtomicU64, Ordering},
};

use crate::chess::{moves::*, zobrist::*};
use crate::engine::search_params::*;

/// TTFlag: determines the type of eval stored in the field
#[repr(u8)]
#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Debug, Hash, Default)]
pub enum TTFlag {
    #[default]
    None, // None, ignore whatever eval value this entry has.
    Lower, // Lower bound, generated by Fail-High (Beta cutoff)
    Upper, // Upper bound, generated by Fail-Low
    Exact, // Exact bound, generated by PV nodes
}

/// TTEntry: uncompressed external representation of tt entries
///
/// Compresses to 128b, every single one is used.
/// Mate scores are normalized within the tt for retrieval at different plies: within the tree, we
/// normalize them to the root-distance, while in the tt they are normalized to node-distance
///
/// 00000000 00000000 00000000 00000000 00000000 00000000 00000000 01111111 -- AGE
/// 00000000 00000000 00000000 00000000 00000000 00000000 00111111 10000000 -- DEPTH
/// 00000000 00000000 00000000 00000000 00000000 00000000 11000000 00000000 -- FLAG
/// 00000000 00000000 00000000 00000000 11111111 11111111 00000000 00000000 -- MOVE
/// 00000000 00000000 11111111 11111111 00000000 00000000 00000000 00000000 -- SEARCH RESULT
/// 11111111 11111111 00000000 00000000 00000000 00000000 00000000 00000000 -- STATIC EVAL
#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Debug, Hash, Default)]
pub struct TTEntry {
    key: u64,         // 64b
    age: u8,          //  7b
    depth: u8,        //  7b
    flag: TTFlag,     //  2b
    best_move: Move,  // 16b
    eval: i16,        // 16b
    static_eval: i16, // 16b
}

// Masks for the data field
const DEPTH_OFFSET: u64 = 7;
const FLAG_OFFSET: u64 = 14;
const MOVE_OFFSET: u64 = 16;
const SEARCH_OFFSET: u64 = 32;
const EVAL_OFFSET: u64 = 48;

const AGE_MASK: u64 = 0x7F;
const DEPTH_MASK: u64 = 0x3F80;
const FLAG_MASK: u64 = 0xC000;
const MOVE_MASK: u64 = 0xFFFF0000;
const SEARCH_MASK: u64 = 0xFFFF00000000;

/// Convert from external root-distance to internal node-distance
fn to_tt(eval: Eval, ply: usize) -> i16 {
    if eval >= MATE_IN_PLY {
        (eval + ply as Eval) as i16
    } else if eval <= -MATE_IN_PLY {
        (eval - ply as Eval) as i16
    } else {
        eval as i16
    }
}

/// Convert from internal node-distance to external root-distance
fn to_search(eval: i16, ply: usize) -> Eval {
    let eval = eval as Eval;
    let ply = ply as Eval;

    if eval >= MATE_IN_PLY {
        eval - ply
    } else if eval <= -MATE_IN_PLY {
        eval + ply
    } else {
        eval
    }
}

impl TTEntry {
    /// Returns entry depth
    pub fn get_depth(self) -> usize {
        self.depth as usize
    }

    /// Returns entry flag
    pub fn get_flag(self) -> TTFlag {
        self.flag
    }

    /// Returns best move
    pub fn get_move(self) -> Option<Move> {
        if self.best_move != NULL_MOVE {
            Some(self.best_move)
        } else {
            None
        }
    }

    /// Returns the best move if it is a capture
    pub fn get_capture(self) -> Option<Move> {
        if self.best_move != NULL_MOVE && self.best_move.get_type().is_capture() {
            Some(self.best_move)
        } else {
            None
        }
    }

    /// Gets search evaluation while normalizing mate scores
    pub fn get_eval(self, ply: usize) -> Eval {
        to_search(self.eval, ply)
    }

    /// Gets the static evaluation of the position
    pub fn get_static_eval(self) -> Eval {
        self.static_eval as Eval
    }
}

/// Actual TTEntry, compressed down to 16B and split in two atomic entries
/// Since entries split key and data, we use a XOR trick to avoid concurrency problems where the two
/// fields are modified simultaneously. This ensures with a 16-bit checksum that the data is correctly
/// associated to the key.
/// https://www.chessprogramming.org/Shared_Hash_Table
#[derive(Debug, Default)]
struct AtomicField {
    key: AtomicU64,
    data: AtomicU64,
}

/// Convert from external field to compressed internal
impl From<TTEntry> for (u64, u64) {
    fn from(field: TTEntry) -> Self {
        let data: u64 = field.age as u64
            | (field.depth as u64) << DEPTH_OFFSET
            | (field.flag as u64) << FLAG_OFFSET
            | (field.best_move.0 as u64) << MOVE_OFFSET
            | (field.eval as u16 as u64) << SEARCH_OFFSET
            | (field.static_eval as u16 as u64) << EVAL_OFFSET;

        (field.key ^ data, data)
    }
}

/// Convert from compressed internal to external
impl From<(u64, u64)> for TTEntry {
    fn from((key, data): (u64, u64)) -> Self {
        TTEntry {
            key,
            age: (data & AGE_MASK) as u8,
            depth: ((data & DEPTH_MASK) >> DEPTH_OFFSET) as u8,
            flag: unsafe { transmute(((data & FLAG_MASK) >> FLAG_OFFSET) as u8) },
            best_move: Move(((data & MOVE_MASK) >> MOVE_OFFSET) as u16),
            eval: ((data & SEARCH_MASK) >> SEARCH_OFFSET) as i16,
            static_eval: (data >> EVAL_OFFSET) as i16,
        }
    }
}

impl AtomicField {
    /// Atomic read checking that the field contents match the checksum
    fn read(&self, hash: ZHash) -> Option<TTEntry> {
        let checksum = hash.0;
        let key = self.key.load(Ordering::SeqCst);
        let data = self.data.load(Ordering::SeqCst);

        if key ^ checksum == data {
            Some(TTEntry::from((checksum, data)))
        } else {
            None
        }
    }

    /// Simple Atomic read, possibly returning mismatching Key and Data
    fn read_unchecked(&self) -> TTEntry {
        let key = self.key.load(Ordering::SeqCst);
        let data = self.data.load(Ordering::SeqCst);

        TTEntry::from((key, data))
    }

    /// Atomic write to table from a tt field struct
    fn write(&self, entry: TTEntry) {
        let (key, data) = entry.into();

        self.key.store(key, Ordering::SeqCst);
        self.data.store(data, Ordering::SeqCst);
    }
}

/// Main transposition table with 16B atomic entries
pub struct TT {
    table: Vec<AtomicField>,
    age: u8,
}
pub const DEFAULT_SIZE: usize = 16;

// Default to 16 MiB size
impl Default for TT {
    fn default() -> Self {
        let mut tt = TT {
            table: Vec::new(),
            age: 0,
        };
        tt.resize(DEFAULT_SIZE);

        tt
    }
}

impl TT {
    /// Get a key that wraps around the table size, avoiding using Modulo.
    /// https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/
    fn get_key(&self, hash: ZHash) -> usize {
        let key = hash.0 as u128;
        let len = self.table.len() as u128;

        ((key * len) >> 64) as usize
    }

    /// Resize the tt to the given size in MiB
    pub fn resize(&mut self, mb_size: usize) {
        let new_size = (mb_size << 20) / size_of::<AtomicField>();
        self.table.resize_with(new_size, AtomicField::default);
    }

    /// Reset the tt to empty entries
    pub fn clear(&mut self) {
        self.age = 0;
        self.table
            .iter_mut()
            .for_each(|entry| *entry = AtomicField::default());
    }

    /// Increase current age. Entries with older age will be overwritten more easily.
    /// Age is kept to 7 bits to fit in the data field
    pub fn increment_age(&mut self) {
        self.age = (self.age + 1) & 0b01111111; // 7 bit age
    }

    /// Prefetch a cache line containing the entry for the given hash
    /// Implementation from Viridithas
    pub fn prefetch(&self, hash: ZHash) {
        #[cfg(target_arch = "x86_64")]
        unsafe {
            use std::arch::x86_64::{_mm_prefetch, _MM_HINT_T0};

            // get a reference to the entry in the table:
            let tt_index = self.get_key(hash);
            let entry = self.table.get_unchecked(tt_index);

            // prefetch the entry:
            _mm_prefetch((entry as *const AtomicField).cast::<i8>(), _MM_HINT_T0);
        }
    }

    /// Probe tt for entry
    /// UB: so long and we use the wrapped key from TT::get_key, we are guaranteed to be within bounds
    pub fn probe(&self, hash: ZHash) -> Option<TTEntry> {
        unsafe { self.table.get_unchecked(self.get_key(hash)).read(hash) }
    }

    /// Insert entry in appropriate tt field.
    ///
    /// Uses highest depth + aging for replacement, but takes special care of different flag types
    /// to maintain the pv within the transposition table:
    ///     - highest priority is given to entry age
    ///     - second highest is given to Exact entries: they only get replaced by better exact ones
    ///     - inexact entries get overwritten by exact entries or entries at the same depth. this
    ///       is because in the search_root function we may replace the same entry many times.
    ///
    /// Conditions are explained here:
    /// https://stackoverflow.com/questions/37782131/chess-extracting-the-principal-variation-from-the-transposition-table
    #[rustfmt::skip]
    #[allow(clippy::too_many_arguments)]
    pub fn insert(
        &self,
        hash: ZHash,
        flag: TTFlag,
        best_move: Move,
        eval: Eval,
        static_eval: Eval,
        depth: usize,
        ply: usize
    ) {
        let old_slot = unsafe { self.table.get_unchecked(self.get_key(hash)) };
        let old  = old_slot.read_unchecked();

        if  self.age != old.age // always replace entries with a different age
            || old.depth == 0   // always replace qsearch/empty entries
            || ((old.flag != TTFlag::Exact && (flag == TTFlag::Exact || depth >= old.depth as usize))
            || (old.flag == TTFlag::Exact && flag == TTFlag::Exact && depth > old.depth as usize))
        {
            // Normalize search eval to node distance
            let eval = to_tt(eval, ply);

            // Don't overwrite best moves with null moves
            let best_move = if best_move != NULL_MOVE {
                best_move
            } else {
                old.best_move
            };

            old_slot.write(TTEntry {
                key: hash.0,
                age: self.age,
                depth: depth as u8,
                flag,
                best_move,
                eval,
                static_eval: static_eval as i16,
            });
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_tt_init() {
        let mut tt = TT::default();
        tt.resize(1); // 1 MiB table -> 2^20 / 2^4 = 65536

        assert_eq!(16, size_of::<AtomicField>());
        assert_eq!(65536, tt.table.len());
    }

    #[test]
    fn test_tt_insert() {
        let tt = TT::default();
        let z = ZHash(0);

        tt.insert(z, TTFlag::Exact, Move(1), 100, 100, 1, 0); // insert in empty field
        tt.insert(z, TTFlag::Exact, Move(1), 100, 100, 2, 0); // replace
        tt.insert(z, TTFlag::Exact, Move(1), 100, 100, 1, 0); // do not replace

        let target1 = tt.probe(z).unwrap();
        let target2 = tt.probe(ZHash(8));

        assert_eq!(2, target1.get_depth());
        assert!(target2.is_none());
    }

    #[test]
    fn test_tt_collision() {
        let mut tt = TT::default();
        tt.resize(1);

        tt.insert(ZHash(0), TTFlag::Exact, NULL_MOVE, 100, 100, 1, 0); // insert field 1
        tt.insert(ZHash(1), TTFlag::Exact, NULL_MOVE, 100, 100, 2, 0); // insert field 2 in same slot as field 1, replacing it

        let new = tt.probe(ZHash(0)); // check no match on first hash
        assert!(new.is_none());
    }
}
